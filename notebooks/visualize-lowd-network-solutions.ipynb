{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import islice, product\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from sympy import parse_expr, lambdify\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# import seml\n",
    "# from sacred import Experiment\n",
    "# from sacred.utils import apply_backspaces_and_linefeeds\n",
    "# ex = Experiment(\"feynman-network-subexperiment-v2-simplexcompare\")\n",
    "# ex.captured_out_filter = apply_backspaces_and_linefeeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters(width, depth, dimension):\n",
    "    \"\"\"Computes number of parameters in MLP with widths `width`,\n",
    "    depth `depth`, and input dimension `dimension`. Assumes the neurons\n",
    "    have biases.\n",
    "    \"\"\"\n",
    "    return (dimension * width + width) + (depth - 2)*(width * width + width) + (width + 1)\n",
    "\n",
    "def width_given_ddp(depth, dimension, parameters):\n",
    "    \"\"\"Given the network depth and input dimension, computes the\n",
    "    width such that the architecture (with bias) has `parameters` parameters.\n",
    "    \"\"\"\n",
    "    if depth == 2:\n",
    "        return int((parameters - 1) / (dimension + 2))\n",
    "    root = (-(dimension + depth) + np.sqrt(np.power(dimension + depth, 2) - 4 * (depth - 2) * (1 - parameters))) / (2 * (depth - 2))\n",
    "    return int(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ex.post_run_hook\n",
    "# def collect_stats(_run):\n",
    "#     seml.collect_exp_stats(_run)\n",
    "\n",
    "# # --------------------------\n",
    "# #    ,-------------.\n",
    "# #   (_\\  CONFIG     \\\n",
    "# #      |    OF      |\n",
    "# #      |    THE     |\n",
    "# #     _| EXPERIMENT |\n",
    "# #    (_/_____(*)___/\n",
    "# #             \\\\\n",
    "# #              ))\n",
    "# #              ^\n",
    "# # --------------------------\n",
    "# @ex.config\n",
    "# def cfg():\n",
    "#     eqn = 'I.10.7'\n",
    "#     width = 100\n",
    "#     depth = 3\n",
    "#     lr = 1e-3\n",
    "#     activation = 'ReLU'\n",
    "#     N_TEST_POINTS = 30000\n",
    "#     TEST_COMPACTIFICATION = 0.8\n",
    "#     MAX_TRAIN_ITERS = 25000\n",
    "#     MAX_BATCH_SIZE = 30000\n",
    "#     spreadsheet = \"/om2/user/ericjm/precision-ml/equations.csv\"\n",
    "#     device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#     dtype = torch.float64\n",
    "\n",
    "#     overwrite = None\n",
    "#     db_collection = None\n",
    "#     if db_collection is not None:\n",
    "#         ex.observers.append(seml.create_mongodb_observer(db_collection, overwrite=overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eqn = 'Z.001'\n",
    "width = 100\n",
    "depth = 3\n",
    "lr = 1e-3\n",
    "activation = 'ReLU'\n",
    "N_TEST_POINTS = 30000\n",
    "TEST_COMPACTIFICATION = 0.8\n",
    "MAX_TRAIN_ITERS = 25000\n",
    "MAX_BATCH_SIZE = 30000\n",
    "spreadsheet = \"/om2/user/ericjm/precision-ml/equations.csv\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float64\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(dtype)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "equations = pd.read_csv(spreadsheet)\n",
    "row = equations[equations['Equation'] == eqn].iloc[0]\n",
    "dimension = int(row['# variables'])\n",
    "formula = row['Formula']\n",
    "variables = [row[f'v{i}_name'] for i in range(1, dimension+1)]\n",
    "ranges = [(row[f'v{i}_low'], row[f'v{i}_high']) for i in range(1, dimension+1)]\n",
    "target = lambdify(variables, parse_expr(formula))\n",
    "\n",
    "\n",
    "TRAIN_POINTS = parameters(width, depth, dimension) // (dimension + 1)\n",
    "# ex.info['TRAIN_POINTS'] = TRAIN_POINTS\n",
    "# _log.debug(f\"TRAIN_POINTS: {TRAIN_POINTS}\")\n",
    "\n",
    "# create datasets\n",
    "ls = np.array([ranges[i][0] for i in range(dimension)])\n",
    "hs = np.array([ranges[i][1] for i in range(dimension)])\n",
    "xs_train = np.random.uniform(low=ls, high=hs, size=(TRAIN_POINTS, dimension))\n",
    "ys_train = target(*[xs_train[:, i] for i in range(dimension)])\n",
    "\n",
    "cs = (hs + ls) / 2\n",
    "ws = (hs - ls) * TEST_COMPACTIFICATION\n",
    "ls, hs = cs - ws / 2, cs + ws / 2\n",
    "xs_test = np.random.uniform(low=ls, high=hs, size=(N_TEST_POINTS, dimension))\n",
    "ys_test = target(*[xs_test[:, i] for i in range(dimension)])\n",
    "\n",
    "xs_train = torch.from_numpy(xs_train).to(device)\n",
    "ys_train = torch.from_numpy(ys_train).to(device).unsqueeze(dim=1)\n",
    "xs_test = torch.from_numpy(xs_test).to(device)\n",
    "ys_test = torch.from_numpy(ys_test).to(device).unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a965c107e997462cb3242b117a9a7ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/om2/user/ericjm/precision-ml/notebooks/visualize-lowd-network-solutions.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bom/om2/user/ericjm/precision-ml/notebooks/visualize-lowd-network-solutions.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bom/om2/user/ericjm/precision-ml/notebooks/visualize-lowd-network-solutions.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     train_l \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqrt(l)\u001b[39m.\u001b[39mitem()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bom/om2/user/ericjm/precision-ml/notebooks/visualize-lowd-network-solutions.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m     test_l \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqrt(torch\u001b[39m.\u001b[39mmean(torch\u001b[39m.\u001b[39mpow(mlp(xs_test) \u001b[39m-\u001b[39m ys_test, \u001b[39m2\u001b[39m)))\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bom/om2/user/ericjm/precision-ml/notebooks/visualize-lowd-network-solutions.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39mif\u001b[39;00m train_l \u001b[39m<\u001b[39m min_train:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bom/om2/user/ericjm/precision-ml/notebooks/visualize-lowd-network-solutions.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m         min_train \u001b[39m=\u001b[39m train_l\n",
      "File \u001b[0;32m/om2/user/ericjm/miniconda3/envs/torch-cuda/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/om2/user/ericjm/miniconda3/envs/torch-cuda/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/om2/user/ericjm/miniconda3/envs/torch-cuda/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/om2/user/ericjm/miniconda3/envs/torch-cuda/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert xs_train.dtype == dtype\n",
    "assert ys_train.dtype == dtype\n",
    "assert xs_test.dtype == dtype\n",
    "assert ys_test.dtype == dtype\n",
    "\n",
    "if activation == 'ReLU':\n",
    "    activation_fn = nn.ReLU\n",
    "elif activation == 'Tanh':\n",
    "    activation_fn = nn.Tanh\n",
    "elif activation == 'Sigmoid':\n",
    "    activation_fn = nn.Sigmoid\n",
    "else:\n",
    "    assert False, f\"Unrecognized activation function identifier: {activation}\"\n",
    "\n",
    "# create model\n",
    "layers = []\n",
    "for i in range(depth):\n",
    "    if i == 0:\n",
    "        layers.append(nn.Linear(dimension, width))\n",
    "        layers.append(activation_fn())\n",
    "    elif i == depth - 1:\n",
    "        layers.append(nn.Linear(width, 1))\n",
    "    else:\n",
    "        layers.append(nn.Linear(width, width))\n",
    "        layers.append(activation_fn())\n",
    "mlp = nn.Sequential(*layers).to(device)\n",
    "# _log.debug(\"Created model.\")\n",
    "# _log.debug(f\"Model has {sum(t.numel() for t in mlp.parameters())} parameters\") \n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optim = torch.optim.Adam(mlp.parameters(), lr=lr)\n",
    "\n",
    "# ex.info['train'] = list()\n",
    "train = list()\n",
    "# ex.info['test'] = list()\n",
    "test = list()\n",
    "min_train = float('inf')\n",
    "min_test = float('inf')\n",
    "test_at_min_train = float('inf')\n",
    "\n",
    "k = 0\n",
    "for step in tqdm(range(MAX_TRAIN_ITERS)):\n",
    "    optim.zero_grad()\n",
    "    if TRAIN_POINTS <= MAX_BATCH_SIZE:\n",
    "        ys_pred = mlp(xs_train)\n",
    "        l = loss_fn(ys_train, ys_pred)\n",
    "    else:\n",
    "        sample_idx = torch.arange(k, k+MAX_BATCH_SIZE, 1) % TRAIN_POINTS\n",
    "        xs_batch, ys_batch = xs_train[sample_idx], ys_train[sample_idx]\n",
    "        ys_pred = mlp(xs_batch)\n",
    "        l = loss_fn(ys_batch, ys_pred)\n",
    "        k += MAX_BATCH_SIZE\n",
    "    l.backward()\n",
    "    optim.step()\n",
    "    with torch.no_grad():\n",
    "        train_l = torch.sqrt(l).item()\n",
    "        test_l = torch.sqrt(torch.mean(torch.pow(mlp(xs_test) - ys_test, 2))).item()\n",
    "        if train_l < min_train:\n",
    "            min_train = train_l\n",
    "            test_at_min_train = test_l\n",
    "        min_test = test_l if test_l < min_test else min_test\n",
    "        if step % 100 == 0:\n",
    "            # ex.info['train'].append(train_l)\n",
    "            # ex.info['test'].append(test_l)\n",
    "            train.append(train_l)\n",
    "            test.append(test_l)\n",
    "    # if step % (MAX_TRAIN_ITERS // 10) == 0:\n",
    "    #     _log.debug(\"{:.0f}% done with training\".format(step / MAX_TRAIN_ITERS * 100))\n",
    "# ex.info['min_train'] = min_train\n",
    "# ex.info['min_test'] = min_test\n",
    "# ex.info['test_at_min_train'] = test_at_min_train\n",
    "# _log.debug(\"Test loss: {:.3e}\".format(test_at_min_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch-cuda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1c5e02af2dc2c2ee5ab0740bd8c5bc11081560a4bed34560b6f09a4b8261b7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
